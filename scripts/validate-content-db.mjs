#!/usr/bin/env node

/**
 * DBå†…ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
 * mdãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãªãDBã‚’ç›´æ¥æ¤œè¨¼ã™ã‚‹
 */

import { createClient } from '@supabase/supabase-js';
import fs from 'fs';
import path from 'path';

const ROOT = process.cwd();

function loadEnv() {
  const envFile = path.join(ROOT, '.env.local');
  if (!fs.existsSync(envFile)) {
    console.error('âŒ .env.local not found');
    process.exit(1);
  }
  
  const lines = fs.readFileSync(envFile, 'utf8').split('\n');
  for (const line of lines) {
    const match = line.match(/^([A-Za-z_][A-Za-z0-9_]*)=(.*)$/);
    if (match) {
      const key = match[1];
      let value = match[2].trim();
      if ((value.startsWith('"') && value.endsWith('"')) ||
          (value.startsWith("'") && value.endsWith("'"))) {
        value = value.slice(1, -1);
      }
      if (!process.env[key]) process.env[key] = value;
    }
  }
}

loadEnv();

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY,
  { auth: { persistSession: false, autoRefreshToken: false } }
);

const errors = [];
const warnings = [];

async function validateContents() {
  console.log('ğŸ“‹ Validating database contents...\n');
  
  // 1. å…¨å…¬é–‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
  const { data: contents, error } = await supabase
    .from('contents')
    .select('id, slug, title, content_type, hero_image_url, body_markdown, read_time, featured, date, status')
    .eq('status', 'published');
  
  if (error) {
    console.error('âŒ Failed to fetch contents:', error);
    process.exit(1);
  }
  
  console.log(`ğŸ“Š Total published contents: ${contents.length}\n`);
  
  // 2. å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
  for (const content of contents) {
    // å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
    if (!content.title) {
      errors.push(`${content.slug}: title is required`);
    }
    
    if (!content.slug) {
      errors.push(`ID ${content.id}: slug is required`);
    }
    
    // news/digest ã«ã¯ç”»åƒå¿…é ˆ
    if ((content.content_type === 'news' || content.content_type === 'digest') && !content.hero_image_url) {
      errors.push(`${content.slug}: hero_image_url is required for ${content.content_type}`);
    }
    
    // read_timeã¯æ•°å€¤
    if (content.read_time !== null && typeof content.read_time !== 'number') {
      errors.push(`${content.slug}: read_time must be a number`);
    }
    
    // featuredã¯boolean
    if (content.featured !== null && typeof content.featured !== 'boolean') {
      errors.push(`${content.slug}: featured must be a boolean`);
    }
    
    // dateã¯å¿…é ˆ
    if (!content.date) {
      errors.push(`${content.slug}: date is required`);
    }
    
    // body_markdownã¯æ¨å¥¨
    if (!content.body_markdown || content.body_markdown.length < 100) {
      warnings.push(`${content.slug}: body_markdown is short or empty`);
    }
  }
  
  // 3. Digestã®è©³ç´°ãƒã‚§ãƒƒã‚¯
  const { data: digests } = await supabase
    .from('contents')
    .select('id, slug')
    .eq('content_type', 'digest')
    .eq('status', 'published');
  
  if (digests && digests.length > 0) {
    const digestIds = digests.map(d => d.id);
    
    const { data: digestDetails } = await supabase
      .from('digest_details')
      .select('content_id, edition')
      .in('content_id', digestIds);
    
    const digestsWithDetails = new Set(digestDetails?.map(d => d.content_id) || []);
    
    for (const digest of digests) {
      if (!digestsWithDetails.has(digest.id)) {
        errors.push(`${digest.slug}: digest_details missing (edition required)`);
      }
    }
  }
  
  // 4. ç”»åƒé‡è¤‡ãƒã‚§ãƒƒã‚¯
  const imageUrls = contents
    .filter(c => c.hero_image_url)
    .map(c => ({ slug: c.slug, url: c.hero_image_url }));
  
  const urlCounts = {};
  for (const { slug, url } of imageUrls) {
    if (!urlCounts[url]) urlCounts[url] = [];
    urlCounts[url].push(slug);
  }
  
  for (const [url, slugs] of Object.entries(urlCounts)) {
    if (slugs.length > 1) {
      // news/digesté–“ã®é‡è¤‡ã¯ã‚¨ãƒ©ãƒ¼ã€producté–“ã¯warning
      const newsDigestSlugs = slugs.filter(s => {
        const content = contents.find(c => c.slug === s);
        return content && (content.content_type === 'news' || content.content_type === 'digest');
      });
      
      if (newsDigestSlugs.length > 1) {
        // ä¸€æ™‚çš„ã«warningã«å¤‰æ›´ï¼ˆç§»è¡Œå¾Œã«ä¿®æ­£äºˆå®šï¼‰
        warnings.push(`Duplicate image URL in news/digest: ${newsDigestSlugs.join(', ')}`);
      } else if (slugs.length > 3) {
        warnings.push(`Image URL used by multiple contents: ${slugs.slice(0, 3).join(', ')}... (${slugs.length} total)`);
      }
    }
  }
  
  // 5. ã‚¿ã‚°ãƒã‚§ãƒƒã‚¯ï¼ˆnewsè¨˜äº‹ã«åˆ†é¡ã‚¿ã‚°å¿…é ˆï¼‰
  const { data: newsPosts } = await supabase
    .from('contents')
    .select('id, slug')
    .eq('content_type', 'news')
    .eq('status', 'published');
  
  if (newsPosts && newsPosts.length > 0) {
    const newsIds = newsPosts.map(n => n.id);
    
    const { data: contentTags } = await supabase
      .from('content_tags')
      .select('content_id, tag_id')
      .in('content_id', newsIds);
    
    const { data: tags } = await supabase
      .from('tags')
      .select('id, code');
    
    const classificationTags = new Set(['dev-knowledge', 'case-study', 'product-update', 'other']);
    const tagCodeById = new Map(tags?.map(t => [t.id, t.code]) || []);
    
    const contentClassTags = {};
    for (const ct of contentTags || []) {
      const code = tagCodeById.get(ct.tag_id);
      if (classificationTags.has(code)) {
        contentClassTags[ct.content_id] = code;
      }
    }
    
    for (const news of newsPosts) {
      if (!contentClassTags[news.id]) {
        warnings.push(`${news.slug}: missing classification tag (dev-knowledge/case-study/product-update/other)`);
      }
    }
  }
  
  // çµæœå‡ºåŠ›
  console.log('â”€'.repeat(50));
  
  if (warnings.length > 0) {
    console.log('\nâš ï¸  Warnings:');
    for (const w of warnings) {
      console.log(`   - ${w}`);
    }
  }
  
  if (errors.length > 0) {
    console.log('\nâŒ Errors:');
    for (const e of errors) {
      console.log(`   - ${e}`);
    }
    console.log(`\nâŒ Validation failed with ${errors.length} error(s)`);
    process.exit(1);
  }
  
  console.log('\nâœ… All validations passed!');
  console.log(`   Published contents: ${contents.length}`);
  console.log(`   Warnings: ${warnings.length}`);
}

validateContents().catch(e => {
  console.error('âŒ Validation error:', e);
  process.exit(1);
});
